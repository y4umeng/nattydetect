nohup: ignoring input
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: y4umeng (y4umeng-columbia-university). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/yw3809/Projects/nattydetect/src/wandb/run-20241219_095011-78889kf6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-fog-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/y4umeng-columbia-university/nattydetect
wandb: üöÄ View run at https://wandb.ai/y4umeng-columbia-university/nattydetect/runs/78889kf6
Loading data...
Building vocab.
Train set length: 9000
Training start.
Traceback (most recent call last):
  File "/home/yw3809/Projects/nattydetect/src/train.py", line 101, in <module>
    main()
  File "/home/yw3809/Projects/nattydetect/src/train.py", line 66, in main
    optimizer.step()
  File "/home/yw3809/Projects/nattydetect/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yw3809/Projects/nattydetect/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yw3809/Projects/nattydetect/.venv/lib/python3.12/site-packages/torch/optim/sparse_adam.py", line 84, in step
    raise RuntimeError(
RuntimeError: SparseAdam does not support dense gradients, please consider Adam instead
[1;34mwandb[0m: üöÄ View run [33mconfused-fog-2[0m at: [34mhttps://wandb.ai/y4umeng-columbia-university/nattydetect/runs/78889kf6[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241219_095011-78889kf6/logs[0m
